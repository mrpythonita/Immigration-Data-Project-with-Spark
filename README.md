{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "younger-summit",
   "metadata": {},
   "source": [
    "# Analysis of Immigration Data in the United States\n",
    "\n",
    "This is the Capstone for the Data Engineering program at Udacity. This project aims at structuring United States immigration data with the intent of analyzing the following aspects:\n",
    "- <i>Correlation between travel volumes and the entry ports</i>     \n",
    "- <i>Correlation between travel volumes and the variation of dempographics in different cities</i>\n",
    "- <i>The seasonality componente of travelling\n",
    "- <i>The impact of temperatures on travellers' volume</i>\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "There are several datasets available for this study which are listed below. In specific, the full immigration dataset has approximately three million rows. \n",
    "    \n",
    "* **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office and includes the contents of the i94 form on entry to the united states. A data dictionary is included in the workspace.\n",
    "* **countries.csv** : Country codes extracted from the data dictionary\n",
    "* **i94portCodes.csv**: City codes used extracted from the data dictionary\n",
    "\n",
    "* **World Temperature Data**: This data set comes from Kaggle, and includes temperatures from several cities worldwide from 1743 to 2013.\n",
    "* **U.S. City Demographic Data**: This data set comes from OpenSoft, and includes US cities demographics from the US Census Bureau's 2015 American Community Survey.\n",
    "* **Airport Code Table**: Airport codes and corresponding cities.\n",
    "    \n",
    "## Schema\n",
    "\n",
    "<div align= \"justify\">Schema on read refers to an innovative data analysis strategy in new data-handling tools and other more involved database technologies. In schema on read, data is applied to a plan or schema as it is pulled out of a stored location, rather than as it goes in. More and more these days, data is a shared asset among groups of people with differing roles and differing interests who want to get different insights from that data; this approach is especially beneficial When delaing with increasingly large volume of data such as the immigration dataset in this project. With schema-on-read data can be presented in a schema that is adapted best to the queries being issued rather than being limited by a one-size-fits-all schema.</div>\n",
    "\n",
    "<img align=\"left\" src=\"DBdiagram.png\" >\n",
    "The fact table is immigration_fact, and includes the following fields:\n",
    "    \n",
    "* cicid\n",
    "* citizenship_country\n",
    "* residence_country\n",
    "* city\n",
    "* state\n",
    "* arrival_date\n",
    "* departure_date\n",
    "* age\n",
    "* visa_type\n",
    "* detailed_visa_type\n",
    "\n",
    "Dimension tables are time_dim, airport_dim, demographic_dim, temperature_dim, and includes the following fields:\n",
    "\n",
    "**time_dim** (aggretate data by time)\n",
    "* date \n",
    "* year \n",
    "* month \n",
    "* day \n",
    "* week\n",
    "* weekday\n",
    "* year_day\n",
    "\n",
    "**airport_dim** (determines area flow of travellers)\n",
    "* ident\n",
    "* type \n",
    "* name, \n",
    "* elevation_ft \n",
    "* state\n",
    "* municipality \n",
    "* iata_code\n",
    "\n",
    "**demographic_dim** (travellers origin areas demographic)\n",
    "* City \n",
    "* State \n",
    "* median_age \n",
    "* male_population \n",
    "* female_population \n",
    "* total population\n",
    "* foreign_born \n",
    "* average_Household_Size \n",
    "* state_code\n",
    "* Race \n",
    "* Count\n",
    "\n",
    "**temperature_dim**: (temperature data)\n",
    "* date \n",
    "* city\n",
    "* average temperature \n",
    "* average temperature uncertainty \n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Python 3 is the environment utilized with the addition of the following libraries:\n",
    "\n",
    "* __pyspark__ (+ dependencies) \n",
    "* __jupyter__ (+ dependencies) \n",
    "\n",
    "## Installing\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_add\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "## Deployment\n",
    "\n",
    "\n",
    "* Capstone Project Template.ipynb**: Jupyter Notebook that executes the project' steps listed below:\n",
    "\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### _Acknowledgments_\n",
    "\n",
    "* Chapeau to StackOverflow, GitHub, and Udacity Knowledge platforms for providing some guidance code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
